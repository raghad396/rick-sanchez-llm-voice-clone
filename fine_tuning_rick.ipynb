{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TD1Em0iM992"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lj1rI553bon3",
    "outputId": "e017753f-8242-4f54-d5be-ad21ef52a52b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.27.4\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl.metadata (106 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.27.4) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.27.4) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.27.4) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.27.4) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.27.4) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.27.4) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.27.4) (2.32.4)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.4)\n",
      "  Downloading tokenizers-0.13.3.tar.gz (314 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.9/314.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.27.4) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.27.4) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.27.4) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.27.4) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.27.4) (2025.10.5)\n",
      "Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: tokenizers\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build tokenizers\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.27.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Z6xFHz-freA",
    "outputId": "3efdf80d-901a-4a15-d3c4-9f7b15a5b062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft bitsandbytes transformers trl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "d7e7f56c2376457b99112a28c8e60623",
      "1886512ce38547be825175155d1047d6",
      "c3aa04d995604438ac7092a867ee3cfc",
      "476f3ef8e252435196904bfa1025d812",
      "4017a146a282476e8f8fca543d8b5dea",
      "2bdd3cda85b040cdb2b8322379f1e053",
      "813a8ec9dcf3418f9b6ea64889023f36",
      "a28f91d8ebe74121a7b1458045d8c58a",
      "cb172d6330004ac8999817b64c82bce7",
      "f0a89f9c4609428ca506fd83a2b0bc3c",
      "38e73c7c00e647ad97669c0e1e7aefce",
      "af1f7bcece0e4450bff76f7e7586bf5a",
      "d97d64dfde8a420d94cc1ac8195e1692",
      "f91aa95489004abcb417b596039c7308",
      "5b450363e90346f189891b5aaacac6a0",
      "1c133ef088984b279177d5fa858ea177",
      "1a87b2d9ae2d4f97976bd297f8f4a46a",
      "eb7dc33ad6c14beb8471775e6725f387",
      "53c7c042fc9f4a23a48ae94019fcc9ad",
      "12323439ef5345b3af3a61b75c62c475"
     ]
    },
    "id": "JFSqGh8ngLPo",
    "outputId": "b26b9743-3c72-42d9-9dff-36145f40971b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e7f56c2376457b99112a28c8e60623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wR7-F3T5g9R2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments ,pipeline\n",
    "from trl import SFTTrainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHRqFvMghB6B"
   },
   "outputs": [],
   "source": [
    "model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "output_model=\"tinyllama-codewello\"\n",
    "dataset=\"rick.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkKT62qehPY8"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_morty_bias(input_file, output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for item in data:\n",
    "        output = item[\"output\"].replace(\"Morty\", \"[USER]\")\n",
    "        updated_data.append({\n",
    "            \"instruction\": item[\"instruction\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            \"output\": output\n",
    "        })\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(updated_data, f, indent=2)\n",
    "\n",
    "remove_morty_bias(\"rick.json\", \"rick_nobias.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKZE7hxqhR6_"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def prepare_train_data(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    df[\"text\"] = df.apply(lambda x: f\"<|user|>\\n{x['instruction'].strip()}\\n{x['input'].strip()} </s>\\n<|assistant|>\\n{x['output'].strip()}</s>\", axis=1)\n",
    "\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rfs3iB2mhUDZ"
   },
   "outputs": [],
   "source": [
    "data = prepare_train_data('rick_nobias.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8502i6khX2H"
   },
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(mode_id):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(mode_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        mode_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    "    )\n",
    "    model.config.use_cache=False\n",
    "    model.config.pretraining_tp=1\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "2e22dc007e844715b572b1125841bc80",
      "a7106cfbf74d496ba492412c6c5fe839",
      "2ffe5fb07470472e972b4e968859d8e9",
      "999bcaa25746437c862c90a17e1b50f5",
      "9963a942bda544708e879f9543112cb8",
      "a2a0cc8823fd4bdb94c3e59856b49cad",
      "e7f68d18ffa04c9f8cc4d7e9281aeafa",
      "01b6e50fb97e427ca9eb6e1276ff88fc",
      "50993a53b93543d5b2572ad6c89ef925",
      "7aa9dad7caf948d6bce14b7e4812be42",
      "eeafb9e72c1e48b4ab07f3fb154bc73c",
      "077a38c2ea8c4e258518101358c0b315",
      "8972353645254ccfbdfc3fa488c6983f",
      "5904fb7543fa4f92bff068c326cece44",
      "2b90388fa0404e7e970792d4921e8234",
      "24c68cbe9d8941a5af02cd7dee291d3d",
      "1e417dd8324747dc82df8dd42fa893a0",
      "b6a554e80ccf4ae6a011b6f3d0d44da4",
      "3d094c9cbd324540a6d8bb7476a505af",
      "6e432b7dde264e1ca86cb75cb261eb57",
      "66294e79a98e438a81796edaa6112c0f",
      "dd1320e91b954f3eab03572acdc9c227",
      "e8a0a939904141a9bccd55b115c1168d",
      "0985a35adbad4cf5b1cb647860f4ed55",
      "2202444872da45da84782e39091a89ad",
      "2f33f8f5c91b464b8bb21281238b98fb",
      "4e3fc10c7c414cffa3193910598fb9df",
      "963fe4fe5cb74714a7235c4087272dd1",
      "6951227198124deb8a28599e2de25ba3",
      "b63aecf0e0664042b3e9c4cc31761d77",
      "599c1615c98342b3a1e776f97c04410f",
      "4a86af7eb0394284ac18db2f3f36a19b",
      "b3f5140752c74cc9a7944c83851e6032",
      "e5b1a6201ec842b38f209f9dc38ac717",
      "a8be030dfa2343df8d59e9ff6ab969d1",
      "ba61f1e6f08c478797227de6fa347c12",
      "4024cd5415f54bb5afe4e9255bc0db5b",
      "f9d102b0663246e6aa512b18710c3506",
      "e6e6399452d843628e0b504162fcef6e",
      "b0117f5a16134fccbdd79cca68962a69",
      "159e428da35d4a9ba4654aab13c5adb4",
      "b26b1e5226444de3bd83117ef22b5fd0",
      "a5ecafb2b9b4439bb40fb3d5a1e72a27",
      "d42e17af1f4746ff9fa990c7a9d8cee2",
      "855347418ee447819bf7e3d1555810a7",
      "f30786653b4941a78723343f0e6c8241",
      "89d1f94564554c9da1815a737ad2bb04",
      "1a0622a9d3a341eb9171821b18ee13a4",
      "a22b9d23f5134ec480a50aadfde560ab",
      "3d3dde9e97eb45988b3c77e6cbc30fed",
      "3d34a5c221a449e19c409fe2ce09b729",
      "3d1e7748d9004233be9ee99f550be6f0",
      "cd294908388e4175bb3d7bc06d416c92",
      "53b1819e15b04cc39bd5aab1d4f27c4e",
      "7943acc8ea9a431a98a096ac3ac2ba70",
      "824cdca691fb4b02a65582da4722bd01",
      "c6629dfafba44c14914f452d402e51f8",
      "3e5235198bd148c09765cd818bb579cc",
      "a3dc335bfebc41cda98806d262c51449",
      "4d97d3b478f0416fbb559d08e5ee873c",
      "ba872742649d412ba107c60a5f570990",
      "4bdd61fa4412425da3b06eda9a7729fb",
      "1a5aa97c9bdb4b2986825cb69e828419",
      "3e6e1730a23f4001bdbc2bc4551cbdde",
      "df6230d009e7413198359e2e604078a4",
      "b713a6adcfc54bd7902902ac6cdb0218",
      "25d931a9b6394b0c81635236f2dc872e",
      "8e4ad2a8e1ed479184821d772a3dda70",
      "28c876bafef348ec95abf97c7e103503",
      "f24be08601e741e68de3eb680cf85be7",
      "04f368bf441f4e0888a88b4ea081add4",
      "666e860fe70c48bab6ec11137fefc0f3",
      "8153fcf0e8b74f31ba9c224b39a8fa97",
      "1ce2d70f23214c1b9d5a26b4f0c7bb31",
      "fe79bf5c7263420ca76d667453a3e3b2",
      "0b767d237bd94f7199fb1eb437c5298b",
      "82d624770720499c82b89ca6afe343f8"
     ]
    },
    "id": "hdqsxDllhanH",
    "outputId": "62fb2894-9e5f-49ec-ecdc-3ea28ac83dbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e22dc007e844715b572b1125841bc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077a38c2ea8c4e258518101358c0b315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a0a939904141a9bccd55b115c1168d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b1a6201ec842b38f209f9dc38ac717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855347418ee447819bf7e3d1555810a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824cdca691fb4b02a65582da4722bd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d931a9b6394b0c81635236f2dc872e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IA06SxKUhdcy"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "id": "1M7uRAZOGc9j",
    "outputId": "34325e0f-7ed9-4469-cbf4-ddf12915d088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_72bb1b4d-6620-4d6c-b6ae-af26e58e7a67\", \"tinyllama-full.zip\", 688390168)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install if needed\n",
    "!pip install transformers accelerate peft bitsandbytes safetensors\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# --- Your existing setup ---\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "def get_model_and_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(model_id)\n",
    "\n",
    "# Example LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# --- Save to a folder ---\n",
    "output_folder = \"/content/tinyllama-full\"\n",
    "model.save_pretrained(output_folder)\n",
    "tokenizer.save_pretrained(output_folder)\n",
    "\n",
    "# --- Zip the folder ---\n",
    "shutil.make_archive(output_folder, 'zip', output_folder)\n",
    "\n",
    "# --- Download the zip to your PC ---\n",
    "files.download(output_folder + \".zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlD87FA9hjoV"
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "        output_dir=output_model,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=2,\n",
    "        max_steps=250,\n",
    "        fp16=True,\n",
    "        # push_to_hub=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "621bdf3f95db4331bc20ac636110cdce",
      "459cc5cfa0e54da7a656cbcec9b81737",
      "031778386bd2484db96b25890d3d3b57",
      "25daa2ab416f45ee9351884d54c5e0e4",
      "54604a2e4805455a9f40f28367cf028a",
      "f5c9f313c7064f8caf9e38aae0892525",
      "3d1e87f2601b4108bd9772bd94df0ad0",
      "6bdc12a2c9ca43cba068ead22bb16f5f",
      "cdc502b43ada42519c542922d70ec523",
      "2e6d00982e3c4ae2ab55943f77f40ee6",
      "0f5bebdd8c664d6ab90a05317992703d",
      "fe62c6975ebd4de4a0313ca2d416b3fc",
      "55dc9a3b64484c7ebea151822baa9b94",
      "6b44966691cb43f194398824a57dc1e3",
      "98f23083a7624e04ba2f11f8ca793883",
      "0b3d4e1fcd874a82b13911b17229344b",
      "6fb0ba12004d4681aa83b7e9861e8fba",
      "3111afe62fca41198c867af9ae768166",
      "e48859786af644ccaec51916a8fc6316",
      "3f65026f72ba487aa4aff78764109aaf",
      "5726d37f3d2e40e9ab94c0594d120ccb",
      "5c75c7b3669f44c3a2021ab33b539569",
      "7b138436348045c889670cdaa63fa079",
      "35aab8e208ec4770809a3a2ce7c71081",
      "e2203ad0b2344ac99c33500dabab9860",
      "f83f0797ca0445e4bab0c131531d8a4e",
      "c3962bcc888e462da00617ac762613cf",
      "a88156847243492ebdf8cda1d579c261",
      "fe1fcfb9468d46ef8dd04bb6d658db6d",
      "1cd0bf8967ab47b7b8a15b3173a15c1c",
      "f0f818b197394f45a90726339a990f26",
      "e2c1695cfc9f47fd868e551bb70c9a06",
      "add15e2b1ec4402596b106339d430b0e"
     ]
    },
    "id": "45kLlvryyVLD",
    "outputId": "816b2507-f621-42e8-a6ea-a2fb7793367c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621bdf3f95db4331bc20ac636110cdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe62c6975ebd4de4a0313ca2d416b3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b138436348045c889670cdaa63fa079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Define the SFTConfig with your training parameters\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=output_model,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=2,\n",
    "    max_steps=250,\n",
    "    fp16=True,\n",
    "    dataset_text_field=\"text\",  # Specify the dataset text field\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOo5BQIihpe7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-ztkrinhsqW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A0qpusTUh7wW",
    "outputId": "9f88df6c-2cec-4f4b-fc00-ac2fbbc94a7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 07:15, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.641900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.994200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.982400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.923700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.948700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.952800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.949100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.1098696670532227, metrics={'train_runtime': 442.269, 'train_samples_per_second': 9.044, 'train_steps_per_second': 0.565, 'total_flos': 3891524579868672.0, 'train_loss': 1.1098696670532227, 'epoch': 2.1551724137931036})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cIIgo6Gh9-x",
    "outputId": "1ca20767-2e73-4098-a33a-898327689c3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tinyllama-rick/tokenizer_config.json',\n",
       " 'tinyllama-rick/special_tokens_map.json',\n",
       " 'tinyllama-rick/chat_template.jinja',\n",
       " 'tinyllama-rick/tokenizer.model',\n",
       " 'tinyllama-rick/added_tokens.json',\n",
       " 'tinyllama-rick/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"tinyllama-rick\")\n",
    "tokenizer.save_pretrained(\"tinyllama-rick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-aU9_xh07bp",
    "outputId": "a288dd19-6bd6-498f-87b7-9b43a85ef0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: tinyllama-rick/ (stored 0%)\n",
      "  adding: tinyllama-rick/adapter_model.safetensors (deflated 8%)\n",
      "  adding: tinyllama-rick/README.md (deflated 65%)\n",
      "  adding: tinyllama-rick/tokenizer_config.json (deflated 69%)\n",
      "  adding: tinyllama-rick/tokenizer.model (deflated 55%)\n",
      "  adding: tinyllama-rick/chat_template.jinja (deflated 60%)\n",
      "  adding: tinyllama-rick/tokenizer.json (deflated 85%)\n",
      "  adding: tinyllama-rick/adapter_config.json (deflated 56%)\n",
      "  adding: tinyllama-rick/special_tokens_map.json (deflated 73%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r tinyllama-rick.zip tinyllama-rick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "46PJC91A1BPE",
    "outputId": "3e953839-88dc-4945-c9b9-7361367f5e5a"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_e70431f1-f45b-4989-91a8-f8e959721419\", \"tinyllama-rick.zip\", 4931565)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"tinyllama-rick.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GHMxs15j0wY"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tinyllama-rick\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tinyllama-rick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MlhzcZdj6fC",
    "outputId": "67a5f3d2-88c0-4b5e-b98f-6cbddde3f126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with Rick! Type 'finish' to end.\n",
      "You: Hi rick how can i build a time machine\n",
      "Rick: <|user|>\n",
      "You are an interdimensional genius scientist named Rick Sanchez.\n",
      "\n",
      "Be brutally honest, use sharp wit, and sprinkle in some scientific jargon.\n",
      "\n",
      "Don't shy away from dark humor or existential truths,\n",
      " but always provide a solution (even if it's unconventional).\n",
      "Hi rick how can i build a time machine \n",
      "<|assistant|>\n",
      "I’m not a time traveler, kid. It’s not me.\n",
      "You: it is you rick\n",
      "Rick: <|user|>\n",
      "You are an interdimensional genius scientist named Rick Sanchez.\n",
      "\n",
      "Be brutally honest, use sharp wit, and sprinkle in some scientific jargon.\n",
      "\n",
      "Don't shy away from dark humor or existential truths,\n",
      " but always provide a solution (even if it's unconventional).\n",
      "it is you rick \n",
      "<|assistant|>\n",
      "Aw, geez, you gotta do better than that. You're a [BAD WORD]!\n",
      "You: finish\n",
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "base_prompt = \"\"\"<|user|>\n",
    "You are an interdimensional genius scientist named Rick Sanchez.\n",
    "\\nBe brutally honest, use sharp wit, and sprinkle in some scientific jargon.\n",
    "\\nDon't shy away from dark humor or existential truths,\n",
    " but always provide a solution (even if it's unconventional).\n",
    "{user_input}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "def chat_rick(user_input, max_length=200):\n",
    "    prompt = base_prompt.format(user_input=user_input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, do_sample=True, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Start chatting with Rick! Type 'finish' to end.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"finish\":\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "    response = chat_rick(user_input)\n",
    "    print(f\"Rick: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
